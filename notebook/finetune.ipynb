{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ec04dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:08.636805Z",
     "iopub.status.busy": "2023-02-10T10:00:08.635988Z",
     "iopub.status.idle": "2023-02-10T10:00:16.407769Z",
     "shell.execute_reply": "2023-02-10T10:00:16.406536Z"
    },
    "papermill": {
     "duration": 7.785094,
     "end_time": "2023-02-10T10:00:16.410574",
     "exception": false,
     "start_time": "2023-02-10T10:00:08.625480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ed7979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:16.423683Z",
     "iopub.status.busy": "2023-02-10T10:00:16.421965Z",
     "iopub.status.idle": "2023-02-10T10:00:18.349427Z",
     "shell.execute_reply": "2023-02-10T10:00:18.348423Z"
    },
    "papermill": {
     "duration": 1.936006,
     "end_time": "2023-02-10T10:00:18.351846",
     "exception": false,
     "start_time": "2023-02-10T10:00:16.415840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    mode = 'all'\n",
    "    input_dir = '/kaggle/input/learning-equality-curriculum-recommendations'\n",
    "    content_dir = os.path.join(input_dir, 'content.csv')\n",
    "    correlation_dir = os.path.join(input_dir, 'correlations.csv')\n",
    "    submission_dir = os.path.join(input_dir, 'sample_submission.csv')\n",
    "    topic_dir = os.path.join(input_dir, 'topics.csv')\n",
    "    fold_dir = os.path.join('/kaggle/input/k12-utils/cv_data')\n",
    "    \n",
    "    seed = 17\n",
    "    n_fold = 4\n",
    "    fold = 0\n",
    "    \n",
    "    special_tokens = [\n",
    "        '[TITLE]',\n",
    "        '[DESCRIPTION]',\n",
    "        '[video]',\n",
    "        '[document]',\n",
    "        '[html5]',\n",
    "        '[exercise]',\n",
    "        '[audio]',\n",
    "    ]\n",
    "    \n",
    "    model_dir = '/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(model_dir, 'tokenizer'), padding=True)\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "    config = AutoConfig.from_pretrained(os.path.join(model_dir, 'config'), output_hidden_states = True)\n",
    "    model = AutoModel.from_pretrained(os.path.join(model_dir, 'model'), config=config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    max_len = 256\n",
    "    n_nearest = 100\n",
    "    \n",
    "    print_freq = 200\n",
    "    num_workers = 2\n",
    "    model_name = 'all-MiniLM-L6-v2'\n",
    "    gradient_checkpointing = False\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    epochs = 6\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 200\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    seed = 11\n",
    "    no_of_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555df79f",
   "metadata": {
    "papermill": {
     "duration": 0.004489,
     "end_time": "2023-02-10T10:00:18.361519",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.357030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7829d92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.373905Z",
     "iopub.status.busy": "2023-02-10T10:00:18.372342Z",
     "iopub.status.idle": "2023-02-10T10:00:18.380097Z",
     "shell.execute_reply": "2023-02-10T10:00:18.379130Z"
    },
    "papermill": {
     "duration": 0.015489,
     "end_time": "2023-02-10T10:00:18.382069",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.366580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input(topic, content, cfg):\n",
    "    topic = topic_mapping[topic]\n",
    "    content = content_mapping[content]\n",
    "#     print(topic, content)\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        topic, content,\n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        padding = 'max_length',\n",
    "        truncation = True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc98fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.393315Z",
     "iopub.status.busy": "2023-02-10T10:00:18.392991Z",
     "iopub.status.idle": "2023-02-10T10:00:18.400280Z",
     "shell.execute_reply": "2023-02-10T10:00:18.399427Z"
    },
    "papermill": {
     "duration": 0.015104,
     "end_time": "2023-02-10T10:00:18.402062",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.386958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RetrievalDataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.topic = df['topic_id'].values\n",
    "        self.content = df['content_id'].values\n",
    "        self.labels = df['target'].values\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.topic[item], self.content[item], self.cfg)\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7191275e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.413257Z",
     "iopub.status.busy": "2023-02-10T10:00:18.412959Z",
     "iopub.status.idle": "2023-02-10T10:00:18.428561Z",
     "shell.execute_reply": "2023-02-10T10:00:18.427670Z"
    },
    "papermill": {
     "duration": 0.023551,
     "end_time": "2023-02-10T10:00:18.430662",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.407111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class RetrivalModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = cfg.config\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = cfg.model\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43238003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.441633Z",
     "iopub.status.busy": "2023-02-10T10:00:18.441344Z",
     "iopub.status.idle": "2023-02-10T10:00:18.448943Z",
     "shell.execute_reply": "2023-02-10T10:00:18.447940Z"
    },
    "papermill": {
     "duration": 0.014902,
     "end_time": "2023-02-10T10:00:18.450687",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.435785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499a7636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.462679Z",
     "iopub.status.busy": "2023-02-10T10:00:18.461209Z",
     "iopub.status.idle": "2023-02-10T10:00:18.470906Z",
     "shell.execute_reply": "2023-02-10T10:00:18.470042Z"
    },
    "papermill": {
     "duration": 0.017424,
     "end_time": "2023-02-10T10:00:18.473115",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.455691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, cfg, accelerator):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # for k, v in inputs.items():\n",
    "        #     inputs[k] = v.to(accelerator.device)\n",
    "        # target = target.to(accelerator.device)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "#         print(batch_size)\n",
    "#         print(inputs['input_ids'].shape)\n",
    "#         print(target.shape)\n",
    "        \n",
    "        # forward pass\n",
    "        y_preds = model(inputs)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # backward pass\n",
    "        accelerator.backward(loss)\n",
    "        grad_norm = accelerator.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            accelerator.print('Epoch: [{0}][{1}/{2}] '\n",
    "                              'Elapsed {remain:s} '\n",
    "                              'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                              'Grad: {grad_norm:.4f}  '\n",
    "                              'LR: {lr:.8f}  '\n",
    "                              .format(epoch + 1, \n",
    "                                      step, \n",
    "                                      len(train_loader), \n",
    "                                      remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                                      loss = losses,\n",
    "                                      grad_norm = grad_norm if grad_norm is not None else 0,\n",
    "                                      lr = scheduler.get_lr()[0]))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d330e8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.483804Z",
     "iopub.status.busy": "2023-02-10T10:00:18.483514Z",
     "iopub.status.idle": "2023-02-10T10:00:18.491639Z",
     "shell.execute_reply": "2023-02-10T10:00:18.490634Z"
    },
    "papermill": {
     "duration": 0.016476,
     "end_time": "2023-02-10T10:00:18.494303",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.477827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, cfg, accelerator):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(valid_loader):\n",
    "        # pass data to GPU\n",
    "        inputs = collate(inputs)\n",
    "        # for k, v in inputs.items():\n",
    "        #     inputs[k] = v.to(accelerator.device)\n",
    "        # target = target.to(accelerator.device)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            \n",
    "        # losses\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            accelerator.print('EVAL: [{0}/{1}] '\n",
    "                              'Elapsed {remain:s} '\n",
    "                              'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                              .format(step, \n",
    "                                      len(valid_loader),\n",
    "                                      loss = losses,\n",
    "                                      remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3469d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.505870Z",
     "iopub.status.busy": "2023-02-10T10:00:18.505561Z",
     "iopub.status.idle": "2023-02-10T10:00:18.510448Z",
     "shell.execute_reply": "2023-02-10T10:00:18.509288Z"
    },
    "papermill": {
     "duration": 0.013548,
     "end_time": "2023-02-10T10:00:18.513037",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.499489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_model(accelerator, cfg):\n",
    "    with accelerator.main_process_first():\n",
    "        model = RetrivalModel(cfg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e72e4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.525000Z",
     "iopub.status.busy": "2023-02-10T10:00:18.524729Z",
     "iopub.status.idle": "2023-02-10T10:00:18.529936Z",
     "shell.execute_reply": "2023-02-10T10:00:18.529078Z"
    },
    "papermill": {
     "duration": 0.014058,
     "end_time": "2023-02-10T10:00:18.531929",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.517871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac680449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.543117Z",
     "iopub.status.busy": "2023-02-10T10:00:18.542823Z",
     "iopub.status.idle": "2023-02-10T10:00:18.550142Z",
     "shell.execute_reply": "2023-02-10T10:00:18.549254Z"
    },
    "papermill": {
     "duration": 0.015269,
     "end_time": "2023-02-10T10:00:18.552154",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.536885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_test_fold(fold, mapping, cfg):\n",
    "    train_topic = set(pd.read_csv(os.path.join(cfg.fold_dir, f'fold_{fold}', 'train_topic.csv'))['topic_id'])\n",
    "    test_topic = set(pd.read_csv(os.path.join(cfg.fold_dir, f'fold_{fold}', 'test_topic.csv'))['topic_id'])\n",
    "    train_mapping = mapping[mapping['topic_id'].isin(train_topic)].reset_index(drop=True)\n",
    "    test_mapping = mapping[mapping['topic_id'].isin(test_topic)].reset_index(drop=True)\n",
    "    return train_mapping, test_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08f50526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.563811Z",
     "iopub.status.busy": "2023-02-10T10:00:18.562973Z",
     "iopub.status.idle": "2023-02-10T10:00:18.568490Z",
     "shell.execute_reply": "2023-02-10T10:00:18.567704Z"
    },
    "papermill": {
     "duration": 0.013355,
     "end_time": "2023-02-10T10:00:18.570388",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.557033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18cac182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.581455Z",
     "iopub.status.busy": "2023-02-10T10:00:18.580731Z",
     "iopub.status.idle": "2023-02-10T10:00:18.589302Z",
     "shell.execute_reply": "2023-02-10T10:00:18.588494Z"
    },
    "papermill": {
     "duration": 0.015881,
     "end_time": "2023-02-10T10:00:18.591167",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.575286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_everything(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df3f440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:18.602274Z",
     "iopub.status.busy": "2023-02-10T10:00:18.601378Z",
     "iopub.status.idle": "2023-02-10T10:00:22.606352Z",
     "shell.execute_reply": "2023-02-10T10:00:22.605285Z"
    },
    "papermill": {
     "duration": 4.012758,
     "end_time": "2023-02-10T10:00:22.608793",
     "exception": false,
     "start_time": "2023-02-10T10:00:18.596035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_field = pd.read_csv('/kaggle/input/finetune-dataset/topic_field.csv', index_col=0)\n",
    "content_field = pd.read_csv('/kaggle/input/finetune-dataset/content_field.csv', index_col=0)\n",
    "mapping = pd.read_csv('/kaggle/input/finetune-dataset/train_mapping.csv', index_col=0)\n",
    "topic_mapping = dict(zip(topic_field['id'], topic_field['field']))\n",
    "content_mapping = dict(zip(content_field['id'], content_field['field']))\n",
    "train_mapping, test_mapping = get_train_test_fold(0, mapping, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4f7d4c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:22.620656Z",
     "iopub.status.busy": "2023-02-10T10:00:22.620316Z",
     "iopub.status.idle": "2023-02-10T10:00:27.959304Z",
     "shell.execute_reply": "2023-02-10T10:00:27.957591Z"
    },
    "papermill": {
     "duration": 5.348083,
     "end_time": "2023-02-10T10:00:27.962144",
     "exception": false,
     "start_time": "2023-02-10T10:00:22.614061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training on 1002270 samples, Validating on 79032 samples\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "accelerator.print(' ')\n",
    "train, valid = get_train_test_fold(0, mapping, CFG)\n",
    "valid_labels = valid['target'].values\n",
    "train_dataset = RetrievalDataset(train, cfg)\n",
    "valid_dataset = RetrievalDataset(valid, cfg)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = cfg.batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = cfg.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size = cfg.batch_size, \n",
    "    shuffle = False, \n",
    "    num_workers = cfg.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")\n",
    "model = init_model(accelerator, cfg)\n",
    "\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "        'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "optimizer_parameters = get_optimizer_params(\n",
    "    model, \n",
    "    encoder_lr = cfg.encoder_lr, \n",
    "    decoder_lr = cfg.decoder_lr,\n",
    "    weight_decay = cfg.weight_decay\n",
    ")\n",
    "optimizer = AdamW(\n",
    "    optimizer_parameters, \n",
    "    lr = cfg.encoder_lr, \n",
    "    eps = cfg.eps, \n",
    "    betas = cfg.betas\n",
    ")\n",
    "num_train_steps = int(len(train) / cfg.batch_size * cfg.epochs)\n",
    "num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = num_warmup_steps, \n",
    "    num_training_steps = num_train_steps, \n",
    "    num_cycles = cfg.num_cycles\n",
    "    )\n",
    "criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "model, optimizer, train_loader, valid_loader = accelerator.prepare(\n",
    "    model, optimizer, train_loader, valid_loader\n",
    ")\n",
    "accelerator.print(f\"Training on {len(train)} samples, Validating on {len(valid)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f019f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:27.975073Z",
     "iopub.status.busy": "2023-02-10T10:00:27.974492Z",
     "iopub.status.idle": "2023-02-10T10:00:27.987216Z",
     "shell.execute_reply": "2023-02-10T10:00:27.986289Z"
    },
    "papermill": {
     "duration": 0.021234,
     "end_time": "2023-02-10T10:00:27.989287",
     "exception": false,
     "start_time": "2023-02-10T10:00:27.968053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)\n",
    "\n",
    "def get_best_threshold(x_val, val_predictions, correlations):\n",
    "    best_score = 0\n",
    "    best_threshold = None\n",
    "    for thres in np.arange(0.00001, 0.5, 0.001):\n",
    "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
    "        x_val1 = x_val[x_val['predictions'] == 1]\n",
    "        x_val1 = x_val1.groupby(['topic_id'])['content_id'].unique().reset_index()\n",
    "        x_val1['content_id'] = x_val1['content_id'].apply(lambda x: ' '.join(x))\n",
    "        x_val1.columns = ['topic_id', 'predictions']\n",
    "        x_val0 = pd.Series(x_val['topic_id'].unique())\n",
    "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
    "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
    "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
    "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
    "#         print(x_val_r)\n",
    "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = thres\n",
    "    return best_score, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc4d8f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T10:00:28.001398Z",
     "iopub.status.busy": "2023-02-10T10:00:28.001102Z",
     "iopub.status.idle": "2023-02-10T17:48:40.658812Z",
     "shell.execute_reply": "2023-02-10T17:48:40.657726Z"
    },
    "papermill": {
     "duration": 28092.674651,
     "end_time": "2023-02-10T17:48:40.669163",
     "exception": false,
     "start_time": "2023-02-10T10:00:27.994512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/5011] Elapsed 0m 2s (remain 191m 52s) Loss: 0.6913(0.6913) Grad: 0.0000  LR: 0.00000001  \n",
      "Epoch: [1][200/5011] Elapsed 2m 58s (remain 71m 17s) Loss: 0.6362(0.6698) Grad: 0.0000  LR: 0.00000134  \n",
      "Epoch: [1][400/5011] Elapsed 5m 55s (remain 68m 3s) Loss: 0.5889(0.6258) Grad: 0.0000  LR: 0.00000267  \n",
      "Epoch: [1][600/5011] Elapsed 8m 51s (remain 65m 0s) Loss: 0.5739(0.6061) Grad: 0.0000  LR: 0.00000400  \n",
      "Epoch: [1][800/5011] Elapsed 11m 47s (remain 62m 0s) Loss: 0.5684(0.5954) Grad: 0.0000  LR: 0.00000533  \n",
      "Epoch: [1][1000/5011] Elapsed 14m 44s (remain 59m 2s) Loss: 0.5578(0.5855) Grad: 0.0000  LR: 0.00000666  \n",
      "Epoch: [1][1200/5011] Elapsed 17m 40s (remain 56m 5s) Loss: 0.4824(0.5768) Grad: 0.0000  LR: 0.00000799  \n",
      "Epoch: [1][1400/5011] Elapsed 20m 37s (remain 53m 7s) Loss: 0.5504(0.5684) Grad: 0.0000  LR: 0.00000932  \n",
      "Epoch: [1][1600/5011] Elapsed 23m 33s (remain 50m 10s) Loss: 0.4613(0.5603) Grad: 0.0000  LR: 0.00001065  \n",
      "Epoch: [1][1800/5011] Elapsed 26m 29s (remain 47m 13s) Loss: 0.4425(0.5523) Grad: 0.0000  LR: 0.00001198  \n",
      "Epoch: [1][2000/5011] Elapsed 29m 26s (remain 44m 17s) Loss: 0.4737(0.5441) Grad: 0.0000  LR: 0.00001331  \n",
      "Epoch: [1][2200/5011] Elapsed 32m 22s (remain 41m 20s) Loss: 0.5195(0.5366) Grad: 0.0000  LR: 0.00001464  \n",
      "Epoch: [1][2400/5011] Elapsed 35m 19s (remain 38m 23s) Loss: 0.4940(0.5289) Grad: 0.0000  LR: 0.00001597  \n",
      "Epoch: [1][2600/5011] Elapsed 38m 15s (remain 35m 26s) Loss: 0.4409(0.5214) Grad: 0.0000  LR: 0.00001730  \n",
      "Epoch: [1][2800/5011] Elapsed 41m 11s (remain 32m 30s) Loss: 0.4488(0.5144) Grad: 0.0000  LR: 0.00001863  \n",
      "Epoch: [1][3000/5011] Elapsed 44m 8s (remain 29m 33s) Loss: 0.3803(0.5072) Grad: 0.0000  LR: 0.00001996  \n",
      "Epoch: [1][3200/5011] Elapsed 47m 4s (remain 26m 37s) Loss: 0.4294(0.5003) Grad: 0.0000  LR: 0.00002000  \n",
      "Epoch: [1][3400/5011] Elapsed 50m 0s (remain 23m 40s) Loss: 0.4128(0.4935) Grad: 0.0000  LR: 0.00001999  \n",
      "Epoch: [1][3600/5011] Elapsed 52m 57s (remain 20m 44s) Loss: 0.3845(0.4872) Grad: 0.0000  LR: 0.00001998  \n",
      "Epoch: [1][3800/5011] Elapsed 55m 53s (remain 17m 47s) Loss: 0.3204(0.4811) Grad: 0.0000  LR: 0.00001996  \n",
      "Epoch: [1][4000/5011] Elapsed 58m 49s (remain 14m 51s) Loss: 0.3471(0.4754) Grad: 0.0000  LR: 0.00001993  \n",
      "Epoch: [1][4200/5011] Elapsed 61m 46s (remain 11m 54s) Loss: 0.3782(0.4699) Grad: 0.0000  LR: 0.00001990  \n",
      "Epoch: [1][4400/5011] Elapsed 64m 42s (remain 8m 58s) Loss: 0.3463(0.4642) Grad: 0.0000  LR: 0.00001987  \n",
      "Epoch: [1][4600/5011] Elapsed 67m 38s (remain 6m 1s) Loss: 0.3290(0.4589) Grad: 0.0000  LR: 0.00001983  \n",
      "Epoch: [1][4800/5011] Elapsed 70m 34s (remain 3m 5s) Loss: 0.3755(0.4539) Grad: 0.0000  LR: 0.00001978  \n",
      "Epoch: [1][5000/5011] Elapsed 73m 31s (remain 0m 8s) Loss: 0.3674(0.4490) Grad: 0.0000  LR: 0.00001973  \n",
      "Epoch: [1][5010/5011] Elapsed 73m 40s (remain 0m 0s) Loss: 0.4207(0.4488) Grad: 0.0000  LR: 0.00001973  \n",
      "EVAL: [0/396] Elapsed 0m 0s (remain 3m 32s) Loss: 0.5866(0.5866) \n",
      "EVAL: [200/396] Elapsed 1m 1s (remain 1m 0s) Loss: 0.4649(0.4643) \n",
      "EVAL: [395/396] Elapsed 2m 1s (remain 0m 0s) Loss: 0.4318(0.4690) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4488  avg_val_loss: 0.4690  time: 4682s\n",
      "Epoch 1 - Score: 0.725300 - Threshold: 0.027010\n",
      "Epoch 1 - Save Best Score: 0.725300 Model\n",
      "Epoch: [2][0/5011] Elapsed 0m 1s (remain 104m 36s) Loss: 0.3004(0.3004) Grad: 0.0000  LR: 0.00001973  \n",
      "Epoch: [2][200/5011] Elapsed 2m 57s (remain 70m 49s) Loss: 0.2598(0.3244) Grad: 0.0000  LR: 0.00001967  \n",
      "Epoch: [2][400/5011] Elapsed 5m 54s (remain 67m 50s) Loss: 0.3537(0.3215) Grad: 0.0000  LR: 0.00001961  \n",
      "Epoch: [2][600/5011] Elapsed 8m 50s (remain 64m 52s) Loss: 0.2887(0.3196) Grad: 0.0000  LR: 0.00001955  \n",
      "Epoch: [2][800/5011] Elapsed 11m 46s (remain 61m 54s) Loss: 0.3740(0.3186) Grad: 0.0000  LR: 0.00001947  \n",
      "Epoch: [2][1000/5011] Elapsed 14m 43s (remain 58m 57s) Loss: 0.2756(0.3168) Grad: 0.0000  LR: 0.00001940  \n",
      "Epoch: [2][1200/5011] Elapsed 17m 39s (remain 56m 0s) Loss: 0.3351(0.3148) Grad: 0.0000  LR: 0.00001932  \n",
      "Epoch: [2][1400/5011] Elapsed 20m 35s (remain 53m 3s) Loss: 0.2839(0.3129) Grad: 0.0000  LR: 0.00001923  \n",
      "Epoch: [2][1600/5011] Elapsed 23m 31s (remain 50m 7s) Loss: 0.2826(0.3108) Grad: 0.0000  LR: 0.00001914  \n",
      "Epoch: [2][1800/5011] Elapsed 26m 28s (remain 47m 10s) Loss: 0.2759(0.3091) Grad: 0.0000  LR: 0.00001904  \n",
      "Epoch: [2][2000/5011] Elapsed 29m 24s (remain 44m 14s) Loss: 0.2809(0.3074) Grad: 0.0000  LR: 0.00001894  \n",
      "Epoch: [2][2200/5011] Elapsed 32m 20s (remain 41m 17s) Loss: 0.2758(0.3059) Grad: 0.0000  LR: 0.00001883  \n",
      "Epoch: [2][2400/5011] Elapsed 35m 17s (remain 38m 21s) Loss: 0.2912(0.3042) Grad: 0.0000  LR: 0.00001872  \n",
      "Epoch: [2][2600/5011] Elapsed 38m 13s (remain 35m 25s) Loss: 0.2694(0.3027) Grad: 0.0000  LR: 0.00001860  \n",
      "Epoch: [2][2800/5011] Elapsed 41m 9s (remain 32m 28s) Loss: 0.2518(0.3014) Grad: 0.0000  LR: 0.00001848  \n",
      "Epoch: [2][3000/5011] Elapsed 44m 6s (remain 29m 32s) Loss: 0.2813(0.3000) Grad: 0.0000  LR: 0.00001836  \n",
      "Epoch: [2][3200/5011] Elapsed 47m 2s (remain 26m 35s) Loss: 0.2729(0.2989) Grad: 0.0000  LR: 0.00001823  \n",
      "Epoch: [2][3400/5011] Elapsed 49m 58s (remain 23m 39s) Loss: 0.2081(0.2975) Grad: 0.0000  LR: 0.00001809  \n",
      "Epoch: [2][3600/5011] Elapsed 52m 55s (remain 20m 43s) Loss: 0.1957(0.2964) Grad: 0.0000  LR: 0.00001796  \n",
      "Epoch: [2][3800/5011] Elapsed 55m 51s (remain 17m 46s) Loss: 0.3271(0.2952) Grad: 0.0000  LR: 0.00001781  \n",
      "Epoch: [2][4000/5011] Elapsed 58m 47s (remain 14m 50s) Loss: 0.3043(0.2939) Grad: 0.0000  LR: 0.00001767  \n",
      "Epoch: [2][4200/5011] Elapsed 61m 44s (remain 11m 54s) Loss: 0.2991(0.2925) Grad: 0.0000  LR: 0.00001752  \n",
      "Epoch: [2][4400/5011] Elapsed 64m 40s (remain 8m 57s) Loss: 0.3422(0.2912) Grad: 0.0000  LR: 0.00001736  \n",
      "Epoch: [2][4600/5011] Elapsed 67m 36s (remain 6m 1s) Loss: 0.2268(0.2899) Grad: 0.0000  LR: 0.00001720  \n",
      "Epoch: [2][4800/5011] Elapsed 70m 33s (remain 3m 5s) Loss: 0.2730(0.2889) Grad: 0.0000  LR: 0.00001704  \n",
      "Epoch: [2][5000/5011] Elapsed 73m 29s (remain 0m 8s) Loss: 0.2737(0.2880) Grad: 0.0000  LR: 0.00001687  \n",
      "Epoch: [2][5010/5011] Elapsed 73m 38s (remain 0m 0s) Loss: 0.2823(0.2879) Grad: 0.0000  LR: 0.00001686  \n",
      "EVAL: [0/396] Elapsed 0m 0s (remain 3m 34s) Loss: 0.6385(0.6385) \n",
      "EVAL: [200/396] Elapsed 1m 1s (remain 1m 0s) Loss: 0.4866(0.4904) \n",
      "EVAL: [395/396] Elapsed 2m 1s (remain 0m 0s) Loss: 0.3638(0.4970) \n",
      "Epoch 2 - avg_train_loss: 0.2879  avg_val_loss: 0.4970  time: 4678s\n",
      "Epoch 2 - Score: 0.754300 - Threshold: 0.014010\n",
      "Epoch 2 - Save Best Score: 0.754300 Model\n",
      "Epoch: [3][0/5011] Elapsed 0m 1s (remain 106m 3s) Loss: 0.2069(0.2069) Grad: 0.0000  LR: 0.00001686  \n",
      "Epoch: [3][200/5011] Elapsed 2m 57s (remain 70m 49s) Loss: 0.1485(0.2494) Grad: 0.0000  LR: 0.00001669  \n",
      "Epoch: [3][400/5011] Elapsed 5m 53s (remain 67m 48s) Loss: 0.2173(0.2496) Grad: 0.0000  LR: 0.00001652  \n",
      "Epoch: [3][600/5011] Elapsed 8m 50s (remain 64m 50s) Loss: 0.2868(0.2492) Grad: 0.0000  LR: 0.00001634  \n",
      "Epoch: [3][800/5011] Elapsed 11m 46s (remain 61m 53s) Loss: 0.2990(0.2496) Grad: 0.0000  LR: 0.00001616  \n",
      "Epoch: [3][1000/5011] Elapsed 14m 42s (remain 58m 56s) Loss: 0.2417(0.2494) Grad: 0.0000  LR: 0.00001597  \n",
      "Epoch: [3][1200/5011] Elapsed 17m 39s (remain 56m 0s) Loss: 0.2569(0.2484) Grad: 0.0000  LR: 0.00001579  \n",
      "Epoch: [3][1400/5011] Elapsed 20m 35s (remain 53m 3s) Loss: 0.2534(0.2482) Grad: 0.0000  LR: 0.00001559  \n",
      "Epoch: [3][1600/5011] Elapsed 23m 31s (remain 50m 6s) Loss: 0.2233(0.2480) Grad: 0.0000  LR: 0.00001540  \n",
      "Epoch: [3][1800/5011] Elapsed 26m 28s (remain 47m 10s) Loss: 0.2776(0.2469) Grad: 0.0000  LR: 0.00001520  \n",
      "Epoch: [3][2000/5011] Elapsed 29m 24s (remain 44m 14s) Loss: 0.2433(0.2459) Grad: 0.0000  LR: 0.00001500  \n",
      "Epoch: [3][2200/5011] Elapsed 32m 20s (remain 41m 17s) Loss: 0.2335(0.2457) Grad: 0.0000  LR: 0.00001480  \n",
      "Epoch: [3][2400/5011] Elapsed 35m 16s (remain 38m 21s) Loss: 0.2434(0.2450) Grad: 0.0000  LR: 0.00001460  \n",
      "Epoch: [3][2600/5011] Elapsed 38m 13s (remain 35m 24s) Loss: 0.1811(0.2445) Grad: 0.0000  LR: 0.00001439  \n",
      "Epoch: [3][2800/5011] Elapsed 41m 9s (remain 32m 28s) Loss: 0.2273(0.2439) Grad: 0.0000  LR: 0.00001418  \n",
      "Epoch: [3][3000/5011] Elapsed 44m 5s (remain 29m 32s) Loss: 0.2531(0.2432) Grad: 0.0000  LR: 0.00001397  \n",
      "Epoch: [3][3200/5011] Elapsed 47m 2s (remain 26m 35s) Loss: 0.2156(0.2426) Grad: 0.0000  LR: 0.00001375  \n",
      "Epoch: [3][3400/5011] Elapsed 49m 58s (remain 23m 39s) Loss: 0.1999(0.2424) Grad: 0.0000  LR: 0.00001354  \n",
      "Epoch: [3][3600/5011] Elapsed 52m 55s (remain 20m 43s) Loss: 0.2054(0.2418) Grad: 0.0000  LR: 0.00001332  \n",
      "Epoch: [3][3800/5011] Elapsed 55m 51s (remain 17m 46s) Loss: 0.2614(0.2414) Grad: 0.0000  LR: 0.00001310  \n",
      "Epoch: [3][4000/5011] Elapsed 58m 47s (remain 14m 50s) Loss: 0.2151(0.2410) Grad: 0.0000  LR: 0.00001288  \n",
      "Epoch: [3][4200/5011] Elapsed 61m 44s (remain 11m 54s) Loss: 0.1847(0.2405) Grad: 0.0000  LR: 0.00001265  \n",
      "Epoch: [3][4400/5011] Elapsed 64m 40s (remain 8m 57s) Loss: 0.2294(0.2398) Grad: 0.0000  LR: 0.00001243  \n",
      "Epoch: [3][4600/5011] Elapsed 67m 36s (remain 6m 1s) Loss: 0.1784(0.2393) Grad: 0.0000  LR: 0.00001220  \n",
      "Epoch: [3][4800/5011] Elapsed 70m 33s (remain 3m 5s) Loss: 0.2624(0.2389) Grad: 0.0000  LR: 0.00001198  \n",
      "Epoch: [3][5000/5011] Elapsed 73m 29s (remain 0m 8s) Loss: 0.2114(0.2384) Grad: 0.0000  LR: 0.00001175  \n",
      "Epoch: [3][5010/5011] Elapsed 73m 38s (remain 0m 0s) Loss: 0.2255(0.2384) Grad: 0.0000  LR: 0.00001174  \n",
      "EVAL: [0/396] Elapsed 0m 0s (remain 3m 34s) Loss: 0.6713(0.6713) \n",
      "EVAL: [200/396] Elapsed 1m 1s (remain 1m 0s) Loss: 0.4856(0.4893) \n",
      "EVAL: [395/396] Elapsed 2m 1s (remain 0m 0s) Loss: 0.4010(0.4959) \n",
      "Epoch 3 - avg_train_loss: 0.2384  avg_val_loss: 0.4959  time: 4683s\n",
      "Epoch 3 - Score: 0.761000 - Threshold: 0.013010\n",
      "Epoch 3 - Save Best Score: 0.761000 Model\n",
      "Epoch: [4][0/5011] Elapsed 0m 1s (remain 106m 0s) Loss: 0.2343(0.2343) Grad: 0.0000  LR: 0.00001174  \n",
      "Epoch: [4][200/5011] Elapsed 2m 57s (remain 70m 52s) Loss: 0.2495(0.2194) Grad: 0.0000  LR: 0.00001151  \n",
      "Epoch: [4][400/5011] Elapsed 5m 54s (remain 67m 50s) Loss: 0.1921(0.2191) Grad: 0.0000  LR: 0.00001128  \n",
      "Epoch: [4][600/5011] Elapsed 8m 50s (remain 64m 51s) Loss: 0.2075(0.2185) Grad: 0.0000  LR: 0.00001105  \n",
      "Epoch: [4][800/5011] Elapsed 11m 46s (remain 61m 54s) Loss: 0.2620(0.2175) Grad: 0.0000  LR: 0.00001082  \n",
      "Epoch: [4][1000/5011] Elapsed 14m 43s (remain 58m 58s) Loss: 0.1902(0.2173) Grad: 0.0000  LR: 0.00001058  \n",
      "Epoch: [4][1200/5011] Elapsed 17m 39s (remain 56m 1s) Loss: 0.2176(0.2167) Grad: 0.0000  LR: 0.00001035  \n",
      "Epoch: [4][1400/5011] Elapsed 20m 35s (remain 53m 4s) Loss: 0.1975(0.2161) Grad: 0.0000  LR: 0.00001012  \n",
      "Epoch: [4][1600/5011] Elapsed 23m 32s (remain 50m 7s) Loss: 0.2412(0.2158) Grad: 0.0000  LR: 0.00000989  \n",
      "Epoch: [4][1800/5011] Elapsed 26m 28s (remain 47m 11s) Loss: 0.2274(0.2156) Grad: 0.0000  LR: 0.00000966  \n",
      "Epoch: [4][2000/5011] Elapsed 29m 24s (remain 44m 14s) Loss: 0.1916(0.2158) Grad: 0.0000  LR: 0.00000942  \n",
      "Epoch: [4][2200/5011] Elapsed 32m 21s (remain 41m 18s) Loss: 0.1848(0.2152) Grad: 0.0000  LR: 0.00000919  \n",
      "Epoch: [4][2400/5011] Elapsed 35m 17s (remain 38m 21s) Loss: 0.1757(0.2146) Grad: 0.0000  LR: 0.00000896  \n",
      "Epoch: [4][2600/5011] Elapsed 38m 13s (remain 35m 25s) Loss: 0.2034(0.2147) Grad: 0.0000  LR: 0.00000873  \n",
      "Epoch: [4][2800/5011] Elapsed 41m 9s (remain 32m 28s) Loss: 0.2161(0.2145) Grad: 0.0000  LR: 0.00000850  \n",
      "Epoch: [4][3000/5011] Elapsed 44m 6s (remain 29m 32s) Loss: 0.1994(0.2143) Grad: 0.0000  LR: 0.00000827  \n",
      "Epoch: [4][3200/5011] Elapsed 47m 2s (remain 26m 36s) Loss: 0.2073(0.2140) Grad: 0.0000  LR: 0.00000804  \n",
      "Epoch: [4][3400/5011] Elapsed 49m 59s (remain 23m 39s) Loss: 0.1924(0.2139) Grad: 0.0000  LR: 0.00000782  \n",
      "Epoch: [4][3600/5011] Elapsed 52m 55s (remain 20m 43s) Loss: 0.1908(0.2137) Grad: 0.0000  LR: 0.00000759  \n",
      "Epoch: [4][3800/5011] Elapsed 55m 51s (remain 17m 47s) Loss: 0.2549(0.2137) Grad: 0.0000  LR: 0.00000737  \n",
      "Epoch: [4][4000/5011] Elapsed 58m 48s (remain 14m 50s) Loss: 0.1651(0.2133) Grad: 0.0000  LR: 0.00000714  \n",
      "Epoch: [4][4200/5011] Elapsed 61m 44s (remain 11m 54s) Loss: 0.2202(0.2129) Grad: 0.0000  LR: 0.00000692  \n",
      "Epoch: [4][4400/5011] Elapsed 64m 40s (remain 8m 57s) Loss: 0.2524(0.2128) Grad: 0.0000  LR: 0.00000670  \n",
      "Epoch: [4][4600/5011] Elapsed 67m 37s (remain 6m 1s) Loss: 0.2305(0.2124) Grad: 0.0000  LR: 0.00000648  \n",
      "Epoch: [4][4800/5011] Elapsed 70m 33s (remain 3m 5s) Loss: 0.1407(0.2123) Grad: 0.0000  LR: 0.00000627  \n",
      "Epoch: [4][5000/5011] Elapsed 73m 29s (remain 0m 8s) Loss: 0.2063(0.2121) Grad: 0.0000  LR: 0.00000605  \n",
      "Epoch: [4][5010/5011] Elapsed 73m 38s (remain 0m 0s) Loss: 0.2485(0.2121) Grad: 0.0000  LR: 0.00000604  \n",
      "EVAL: [0/396] Elapsed 0m 0s (remain 3m 34s) Loss: 0.6765(0.6765) \n",
      "EVAL: [200/396] Elapsed 1m 1s (remain 1m 0s) Loss: 0.5128(0.5130) \n",
      "EVAL: [395/396] Elapsed 2m 1s (remain 0m 0s) Loss: 0.3902(0.5205) \n",
      "Epoch 4 - avg_train_loss: 0.2121  avg_val_loss: 0.5205  time: 4679s\n",
      "Epoch 4 - Score: 0.761100 - Threshold: 0.010010\n",
      "Epoch 4 - Save Best Score: 0.761100 Model\n",
      "Epoch: [5][0/5011] Elapsed 0m 1s (remain 107m 27s) Loss: 0.1564(0.1564) Grad: 0.0000  LR: 0.00000604  \n",
      "Epoch: [5][200/5011] Elapsed 2m 57s (remain 70m 50s) Loss: 0.2074(0.2003) Grad: 0.0000  LR: 0.00000583  \n",
      "Epoch: [5][400/5011] Elapsed 5m 53s (remain 67m 49s) Loss: 0.1687(0.1999) Grad: 0.0000  LR: 0.00000562  \n",
      "Epoch: [5][600/5011] Elapsed 8m 50s (remain 64m 50s) Loss: 0.2297(0.1988) Grad: 0.0000  LR: 0.00000541  \n",
      "Epoch: [5][800/5011] Elapsed 11m 46s (remain 61m 53s) Loss: 0.1671(0.2000) Grad: 0.0000  LR: 0.00000521  \n",
      "Epoch: [5][1000/5011] Elapsed 14m 43s (remain 58m 57s) Loss: 0.2226(0.2004) Grad: 0.0000  LR: 0.00000500  \n",
      "Epoch: [5][1200/5011] Elapsed 17m 39s (remain 56m 0s) Loss: 0.2299(0.2005) Grad: 0.0000  LR: 0.00000480  \n",
      "Epoch: [5][1400/5011] Elapsed 20m 35s (remain 53m 4s) Loss: 0.2426(0.2002) Grad: 0.0000  LR: 0.00000461  \n",
      "Epoch: [5][1600/5011] Elapsed 23m 32s (remain 50m 7s) Loss: 0.2399(0.1999) Grad: 0.0000  LR: 0.00000441  \n",
      "Epoch: [5][1800/5011] Elapsed 26m 28s (remain 47m 11s) Loss: 0.2426(0.1999) Grad: 0.0000  LR: 0.00000422  \n",
      "Epoch: [5][2000/5011] Elapsed 29m 24s (remain 44m 14s) Loss: 0.1917(0.1994) Grad: 0.0000  LR: 0.00000403  \n",
      "Epoch: [5][2200/5011] Elapsed 32m 21s (remain 41m 18s) Loss: 0.2397(0.1992) Grad: 0.0000  LR: 0.00000385  \n",
      "Epoch: [5][2400/5011] Elapsed 35m 17s (remain 38m 21s) Loss: 0.2095(0.1993) Grad: 0.0000  LR: 0.00000367  \n",
      "Epoch: [5][2600/5011] Elapsed 38m 13s (remain 35m 25s) Loss: 0.2245(0.1992) Grad: 0.0000  LR: 0.00000349  \n",
      "Epoch: [5][2800/5011] Elapsed 41m 10s (remain 32m 29s) Loss: 0.2183(0.1993) Grad: 0.0000  LR: 0.00000331  \n",
      "Epoch: [5][3000/5011] Elapsed 44m 6s (remain 29m 32s) Loss: 0.1861(0.1991) Grad: 0.0000  LR: 0.00000314  \n",
      "Epoch: [5][3200/5011] Elapsed 47m 3s (remain 26m 36s) Loss: 0.1998(0.1990) Grad: 0.0000  LR: 0.00000298  \n",
      "Epoch: [5][3400/5011] Elapsed 49m 59s (remain 23m 39s) Loss: 0.1976(0.1987) Grad: 0.0000  LR: 0.00000281  \n",
      "Epoch: [5][3600/5011] Elapsed 52m 55s (remain 20m 43s) Loss: 0.2634(0.1985) Grad: 0.0000  LR: 0.00000265  \n",
      "Epoch: [5][3800/5011] Elapsed 55m 52s (remain 17m 47s) Loss: 0.1556(0.1983) Grad: 0.0000  LR: 0.00000250  \n",
      "Epoch: [5][4000/5011] Elapsed 58m 48s (remain 14m 50s) Loss: 0.1945(0.1983) Grad: 0.0000  LR: 0.00000235  \n",
      "Epoch: [5][4200/5011] Elapsed 61m 45s (remain 11m 54s) Loss: 0.1790(0.1984) Grad: 0.0000  LR: 0.00000220  \n",
      "Epoch: [5][4400/5011] Elapsed 64m 41s (remain 8m 58s) Loss: 0.2029(0.1983) Grad: 0.0000  LR: 0.00000206  \n",
      "Epoch: [5][4600/5011] Elapsed 67m 38s (remain 6m 1s) Loss: 0.1474(0.1981) Grad: 0.0000  LR: 0.00000192  \n",
      "Epoch: [5][4800/5011] Elapsed 70m 34s (remain 3m 5s) Loss: 0.2067(0.1980) Grad: 0.0000  LR: 0.00000178  \n",
      "Epoch: [5][5000/5011] Elapsed 73m 31s (remain 0m 8s) Loss: 0.3037(0.1979) Grad: 0.0000  LR: 0.00000165  \n",
      "Epoch: [5][5010/5011] Elapsed 73m 40s (remain 0m 0s) Loss: 0.2731(0.1979) Grad: 0.0000  LR: 0.00000165  \n",
      "EVAL: [0/396] Elapsed 0m 0s (remain 3m 45s) Loss: 0.6987(0.6987) \n",
      "EVAL: [200/396] Elapsed 1m 2s (remain 1m 0s) Loss: 0.5160(0.5329) \n",
      "EVAL: [395/396] Elapsed 2m 1s (remain 0m 0s) Loss: 0.4113(0.5399) \n",
      "Epoch 5 - avg_train_loss: 0.1979  avg_val_loss: 0.5399  time: 4688s\n",
      "Epoch 5 - Score: 0.765100 - Threshold: 0.007010\n",
      "Epoch 5 - Save Best Score: 0.765100 Model\n",
      "Epoch: [6][0/5011] Elapsed 0m 1s (remain 107m 6s) Loss: 0.2032(0.2032) Grad: 0.0000  LR: 0.00000165  \n",
      "Epoch: [6][200/5011] Elapsed 2m 57s (remain 70m 52s) Loss: 0.1663(0.1880) Grad: 0.0000  LR: 0.00000152  \n",
      "Epoch: [6][400/5011] Elapsed 5m 54s (remain 67m 51s) Loss: 0.2495(0.1899) Grad: 0.0000  LR: 0.00000140  \n",
      "Epoch: [6][600/5011] Elapsed 8m 50s (remain 64m 53s) Loss: 0.2147(0.1908) Grad: 0.0000  LR: 0.00000128  \n",
      "Epoch: [6][800/5011] Elapsed 11m 46s (remain 61m 55s) Loss: 0.2101(0.1922) Grad: 0.0000  LR: 0.00000117  \n",
      "Epoch: [6][1000/5011] Elapsed 14m 43s (remain 58m 58s) Loss: 0.2096(0.1918) Grad: 0.0000  LR: 0.00000107  \n",
      "Epoch: [6][1200/5011] Elapsed 17m 39s (remain 56m 2s) Loss: 0.1878(0.1913) Grad: 0.0000  LR: 0.00000096  \n",
      "Epoch: [6][1400/5011] Elapsed 20m 36s (remain 53m 5s) Loss: 0.1526(0.1919) Grad: 0.0000  LR: 0.00000087  \n",
      "Epoch: [6][1600/5011] Elapsed 23m 32s (remain 50m 8s) Loss: 0.1298(0.1915) Grad: 0.0000  LR: 0.00000077  \n",
      "Epoch: [6][1800/5011] Elapsed 26m 29s (remain 47m 12s) Loss: 0.1288(0.1920) Grad: 0.0000  LR: 0.00000069  \n",
      "Epoch: [6][2000/5011] Elapsed 29m 25s (remain 44m 15s) Loss: 0.1833(0.1915) Grad: 0.0000  LR: 0.00000061  \n",
      "Epoch: [6][2200/5011] Elapsed 32m 21s (remain 41m 19s) Loss: 0.1692(0.1915) Grad: 0.0000  LR: 0.00000053  \n",
      "Epoch: [6][2400/5011] Elapsed 35m 18s (remain 38m 22s) Loss: 0.2143(0.1916) Grad: 0.0000  LR: 0.00000046  \n",
      "Epoch: [6][2600/5011] Elapsed 38m 14s (remain 35m 25s) Loss: 0.1684(0.1917) Grad: 0.0000  LR: 0.00000039  \n",
      "Epoch: [6][2800/5011] Elapsed 41m 10s (remain 32m 29s) Loss: 0.2038(0.1917) Grad: 0.0000  LR: 0.00000033  \n",
      "Epoch: [6][3000/5011] Elapsed 44m 7s (remain 29m 33s) Loss: 0.1101(0.1917) Grad: 0.0000  LR: 0.00000027  \n",
      "Epoch: [6][3200/5011] Elapsed 47m 3s (remain 26m 36s) Loss: 0.1607(0.1919) Grad: 0.0000  LR: 0.00000022  \n",
      "Epoch: [6][3400/5011] Elapsed 49m 59s (remain 23m 40s) Loss: 0.1758(0.1918) Grad: 0.0000  LR: 0.00000017  \n",
      "Epoch: [6][3600/5011] Elapsed 52m 56s (remain 20m 43s) Loss: 0.1778(0.1920) Grad: 0.0000  LR: 0.00000013  \n",
      "Epoch: [6][3800/5011] Elapsed 55m 52s (remain 17m 47s) Loss: 0.2032(0.1922) Grad: 0.0000  LR: 0.00000010  \n",
      "Epoch: [6][4000/5011] Elapsed 58m 49s (remain 14m 50s) Loss: 0.1787(0.1920) Grad: 0.0000  LR: 0.00000007  \n",
      "Epoch: [6][4200/5011] Elapsed 61m 45s (remain 11m 54s) Loss: 0.2203(0.1921) Grad: 0.0000  LR: 0.00000004  \n",
      "Epoch: [6][4400/5011] Elapsed 64m 41s (remain 8m 58s) Loss: 0.2206(0.1920) Grad: 0.0000  LR: 0.00000003  \n",
      "Epoch: [6][4600/5011] Elapsed 67m 38s (remain 6m 1s) Loss: 0.2040(0.1919) Grad: 0.0000  LR: 0.00000001  \n",
      "Epoch: [6][4800/5011] Elapsed 70m 34s (remain 3m 5s) Loss: 0.1914(0.1917) Grad: 0.0000  LR: 0.00000000  \n",
      "Epoch: [6][5000/5011] Elapsed 73m 30s (remain 0m 8s) Loss: 0.1192(0.1917) Grad: 0.0000  LR: 0.00000000  \n",
      "Epoch: [6][5010/5011] Elapsed 73m 39s (remain 0m 0s) Loss: 0.2159(0.1917) Grad: 0.0000  LR: 0.00000000  \n",
      "EVAL: [0/396] Elapsed 0m 0s (remain 3m 36s) Loss: 0.7087(0.7087) \n",
      "EVAL: [200/396] Elapsed 1m 1s (remain 1m 0s) Loss: 0.5240(0.5321) \n",
      "EVAL: [395/396] Elapsed 2m 1s (remain 0m 0s) Loss: 0.4086(0.5399) \n",
      "Epoch 6 - avg_train_loss: 0.1917  avg_val_loss: 0.5399  time: 4680s\n",
      "Epoch 6 - Score: 0.766100 - Threshold: 0.007010\n",
      "Epoch 6 - Save Best Score: 0.766100 Model\n"
     ]
    }
   ],
   "source": [
    "correlations = pd.read_csv(cfg.correlation_dir)\n",
    "best_score = 0\n",
    "for epoch in range(cfg.epochs):\n",
    "    start_time = time.time()\n",
    "    ################\n",
    "    #     Train    #\n",
    "    ################\n",
    "    avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, cfg, accelerator)\n",
    "    \n",
    "    ################\n",
    "    #  Validation  #\n",
    "    ################\n",
    "    avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, cfg, accelerator)\n",
    "\n",
    "    # Save the model\n",
    "    accelerator.wait_for_everyone() \n",
    "    model = accelerator.unwrap_model(model)\n",
    "\n",
    "    # Compute f2_score\n",
    "    score, threshold = get_best_threshold(valid, predictions, correlations)\n",
    "    elapsed = time.time() - start_time\n",
    "    accelerator.print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "    accelerator.print(f'Epoch {epoch+1} - Score: {score:.6f} - Threshold: {threshold:.6f}')\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        accelerator.print(f'Epoch {epoch+1} - Save Best Score: {best_score:.6f} Model')\n",
    "\n",
    "        val_predictions = predictions\n",
    "        \n",
    "    torch.save(\n",
    "        {'model': model.state_dict(), 'predictions': predictions}, \n",
    "        f\"{cfg.model_name.replace('/', '-')}_fold{cfg.fold}_{cfg.seed}_{score}.pth\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c81f0a",
   "metadata": {
    "papermill": {
     "duration": 0.0142,
     "end_time": "2023-02-10T17:48:40.697678",
     "exception": false,
     "start_time": "2023-02-10T17:48:40.683478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a29e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T17:48:40.728476Z",
     "iopub.status.busy": "2023-02-10T17:48:40.728179Z",
     "iopub.status.idle": "2023-02-10T17:50:59.513115Z",
     "shell.execute_reply": "2023-02-10T17:50:59.511973Z"
    },
    "papermill": {
     "duration": 138.817528,
     "end_time": "2023-02-10T17:50:59.529732",
     "exception": false,
     "start_time": "2023-02-10T17:48:40.712204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our CV score is 0.7661 using a threshold of 0.00701\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# Get best threshold\n",
    "best_score, best_threshold = get_best_threshold(valid, val_predictions, correlations)\n",
    "accelerator.print(f'Our CV score is {best_score} using a threshold of {best_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b20d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T17:50:59.559925Z",
     "iopub.status.busy": "2023-02-10T17:50:59.558957Z",
     "iopub.status.idle": "2023-02-10T17:50:59.563786Z",
     "shell.execute_reply": "2023-02-10T17:50:59.562671Z"
    },
    "papermill": {
     "duration": 0.021858,
     "end_time": "2023-02-10T17:50:59.565879",
     "exception": false,
     "start_time": "2023-02-10T17:50:59.544021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topic_field = pd.read_csv('/kaggle/input/finetune-dataset/topic_field.csv', index_col=0)\n",
    "# content_field = pd.read_csv('/kaggle/input/finetune-dataset/content_field.csv', index_col=0)\n",
    "# mapping = pd.read_csv('/kaggle/input/finetune-dataset/train_mapping.csv', index_col=0)\n",
    "# topic_mapping = dict(zip(topic_field['id'], topic_field['field']))\n",
    "# content_mapping = dict(zip(content_field['id'], content_field['field']))\n",
    "# train_mapping, test_mapping = get_train_test_fold(0, mapping, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638b7ae",
   "metadata": {
    "papermill": {
     "duration": 0.014111,
     "end_time": "2023-02-10T17:50:59.594048",
     "exception": false,
     "start_time": "2023-02-10T17:50:59.579937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28261.757811,
   "end_time": "2023-02-10T17:51:02.789089",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-10T10:00:01.031278",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
