{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfb53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, BertForSequenceClassification\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_STAGE2 = True  # setup False to close Stage2\n",
    "N_NEIGHBOR = 50 # count of Stage1 retrival\n",
    "\n",
    "# INPUT_DIR = '/kaggle/input/learning-equality-curriculum-recommendations'\n",
    "OUTPUT_PATH = '/home/search3/lichunyu/k12-curriculum-recommendations/tmp/output'\n",
    "\n",
    "# TOPIC_DIR = os.path.join(INPUT_DIR, 'topics.csv')\n",
    "# CONTENT_DIR = os.path.join(INPUT_DIR, 'content.csv')\n",
    "# CORR_DIR = os.path.join(INPUT_DIR, 'sample_submission.csv')\n",
    "\n",
    "MODEL_DIR = '/home/search3/lichunyu/k12-curriculum-recommendations/data/input/models/stage1/all-MiniLM-L6-v2/r1_12_38520/checkpoint-38520'\n",
    "TOKENIZER_DIR = '/home/search3/lichunyu/k12-curriculum-recommendations/data/input/models/stage1/all-MiniLM-L6-v2/all-MiniLM-L6-v2/all-MiniLM-L6-v2_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_field(d):\n",
    "    title = list(filter(lambda x: pd.notna(x), d['title_level']))\n",
    "    title = ' of '.join(title[-1::-1])\n",
    "    title = 'No information' if title=='' else title\n",
    "    title = '[TITLE] ' + title + '. '\n",
    "    description = d['description'] if pd.notna(d['description']) else 'No information'\n",
    "    description = '[DESCRIPTION]' + description + '. '\n",
    "    field = title + description\n",
    "    return field\n",
    "\n",
    "def get_content_field(d):\n",
    "    title = d['title']\n",
    "    title = 'No information' if pd.isna(title) else title\n",
    "    title = '[TITLE] ' + title + '. '\n",
    "    description = d['description'] if pd.notna(d['description']) else 'No information'\n",
    "    description = '[DESCRIPTION]' + description + '. '\n",
    "    kind = '[' + d['kind'] + '] '\n",
    "    field = kind + title + description\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b657dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    \n",
    "    def __init__(self, topic_path, content_path, submission_path):\n",
    "        self.topic = pd.read_csv(topic_path)\n",
    "        self.content = pd.read_parquet(content_path)\n",
    "        self.corr = pd.read_parquet(submission_path).drop_duplicates(subset=\"topic_id\").reset_index(drop=True)\n",
    "        # self.topic = self.topic[self.topic['id'].isin(self.corr['topic_id'].to_list())]\n",
    "        self.match_dict = None\n",
    "    \n",
    "    def prepare_topic(self):\n",
    "        df_level = self._get_level_features(self.topic)\n",
    "        self.topic = self.topic.merge(df_level, on='id', how='inner')\n",
    "        self.topic['field'] = self.topic.apply(lambda x: get_topic_field(x), axis=1)\n",
    "        return self.topic\n",
    "    \n",
    "    def prepare_content(self):\n",
    "        self.content['field'] = self.content.apply(lambda x: get_content_field(x), axis=1)\n",
    "        return self.content\n",
    "    \n",
    "    def prepare_language_match(self):\n",
    "        topic = self.topic[['id', 'language']].merge(self.corr, left_on='id', right_on='topic_id', how='right')[['id', 'language']]\n",
    "        match_dict = {}\n",
    "        print(topic['language'].unique())\n",
    "        for language in topic['language'].unique():\n",
    "            match_dict[language] = (topic.query('language==@language')[['id']], self.content.query('language==@language')[['id']])\n",
    "        self.match_dict = match_dict\n",
    "        return match_dict\n",
    "    \n",
    "    \n",
    "    def _get_level_features(self, df_topic, level_cols=['title']):\n",
    "        cols = list(set(level_cols + ['id', 'parent', 'level', 'has_content']))\n",
    "        df_hier = df_topic[cols]\n",
    "        \n",
    "        highest_level = df_hier['level'].max()\n",
    "        print(f'Highest Level: {highest_level}')\n",
    "\n",
    "        df_level = df_hier.query('level == 0').copy(deep=True)\n",
    "        level_list = list()\n",
    "        for col in level_cols:\n",
    "            df_level[f'{col}_level'] = df_level[f'{col}'].apply(lambda x: [x])\n",
    "\n",
    "        for i in tqdm(range(highest_level + 1)):\n",
    "            level_list.append(df_level[df_level['has_content']])\n",
    "            df_level_high = df_hier.query('level == @i+1')\n",
    "            df_level = df_level_high.merge(df_level, left_on='parent', right_on='id', suffixes=['', '_parent'], how='inner')\n",
    "            for col in level_cols:\n",
    "                df_level[f'{col}_level'] = df_level[f'{col}_level'] + df_level[f'{col}'].apply(lambda x: [x])\n",
    "            for col in df_level.columns:\n",
    "                if col.endswith('_parent'):\n",
    "                    df_level.drop(columns=col, inplace=True)\n",
    "        df = pd.concat(level_list).reset_index(drop=True)\n",
    "        return df[set(['id'] + [f'{col}_level' for col in level_cols])]\n",
    "    \n",
    "    def prepare(self):\n",
    "        self.prepare_topic()\n",
    "        self.prepare_content()\n",
    "        self.prepare_language_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ea541",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train\n",
    "# topic_dir = \"/home/search3/lichunyu/k12-curriculum-recommendations/data/input/raw/topics.csv\"\n",
    "# content_dir = \"/home/search3/lichunyu/k12-curriculum-recommendations/data/input/kflod_data/flod0/train_content_flod0.pqt\"\n",
    "# corr_dir = \"/home/search3/lichunyu/k12-curriculum-recommendations/data/input/kflod_data/flod0/train_correlations_flod0.pqt\"\n",
    "\n",
    "# Valid\n",
    "topic_dir = \"/home/search3/lichunyu/k12-curriculum-recommendations/data/input/raw/topics.csv\"\n",
    "content_dir = \"/home/search3/lichunyu/k12-curriculum-recommendations/data/input/kflod_data/flod0/valid_content_flod0.pqt\"\n",
    "corr_dir = \"/home/search3/lichunyu/k12-curriculum-recommendations/data/input/kflod_data/flod0/valid_correlations_flod0_no_source.pqt\"\n",
    "\n",
    "\n",
    "dp = DataPreparation(topic_dir, content_dir, corr_dir)\n",
    "x = dp.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba584b3",
   "metadata": {},
   "source": [
    "# Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e919c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, label_name=\"\") -> None:\n",
    "        super().__init__()\n",
    "        self.data = df[label_name].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        inputs = self.tokenizer(\n",
    "                text, \n",
    "                add_special_tokens = True,\n",
    "                truncation='longest_first',\n",
    "                max_length = 128,\n",
    "                padding = 'max_length',\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt',\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retrieval():\n",
    "    \n",
    "    def __init__(self, model_path, tokenizer_path, dp):\n",
    "        self.model = AutoModel.from_pretrained(model_path).cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        self.topic = dp.topic\n",
    "        self.content = dp.content\n",
    "        self.corr = dp.corr\n",
    "        self.topic_content_match = dp.match_dict\n",
    "    \n",
    "    def convert2embed(self, df, label_name='field'):\n",
    "        embed: list = []\n",
    "        dataset = PlainDataset(df, tokenizer=self.tokenizer, label_name=label_name)\n",
    "        dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=32)\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "                embeddings = self.model(**batch, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                embed.append(embeddings.cpu().clone().detach().numpy())\n",
    "        embed = np.concatenate(embed, axis=0)\n",
    "        return embed\n",
    "    \n",
    "    def get_embed(self):\n",
    "        for lang in self.topic_content_match.keys():\n",
    "            if not isinstance(lang, str):\n",
    "                continue\n",
    "            topic_, content_ = self.topic_content_match[lang]\n",
    "            topic_ = topic_[['id']].merge(self.topic[['id', 'field']], on='id', how='left')\n",
    "            content_ = content_[['id']].merge(self.content[['id', 'field']], on='id', how='left')\n",
    "            \n",
    "            topic_path = os.path.join(OUTPUT_PATH, f\"topic_{lang}.pqt\")\n",
    "            content_path = os.path.join(OUTPUT_PATH, f\"content_{lang}.pqt\")\n",
    "            topic_.to_parquet(topic_path)\n",
    "            content_.to_parquet(content_path)\n",
    "            \n",
    "            for t in [\"content\", \"topic\"]:\n",
    "                path = os.path.join(OUTPUT_PATH, f\"{t}_{lang}.pqt\")\n",
    "                df = pd.read_parquet(path)\n",
    "                embed = self.convert2embed(df, label_name=\"field\")\n",
    "                np.save(path.replace(\".pqt\", \".npy\"), embed)\n",
    "                \n",
    "    def inference(self):\n",
    "        recall_amount = 0\n",
    "        recall_amount_total = 0\n",
    "        recall_num = 0\n",
    "        recall_total = {}\n",
    "        \n",
    "        df_pred_list = []\n",
    "        for lang in self.topic_content_match.keys():\n",
    "            if not isinstance(lang, str):\n",
    "                continue\n",
    "            # global df_pred, df_correlations\n",
    "            content_path = os.path.join(OUTPUT_PATH, f\"content_{lang}.npy\")\n",
    "            topics_path = os.path.join(OUTPUT_PATH, f\"topic_{lang}.npy\")\n",
    "            content_array = np.load(content_path)\n",
    "            topics_array = np.load(topics_path)\n",
    "            \n",
    "            model = NearestNeighbors(n_neighbors=N_NEIGHBOR, metric=\"cosine\")\n",
    "            model.fit(content_array)\n",
    "            d, r = model.kneighbors(topics_array)\n",
    "            df_content = pd.read_parquet(content_path.replace(\".npy\", \".pqt\"))\n",
    "            df_topics = pd.read_parquet(topics_path.replace(\".npy\", \".pqt\"))\n",
    "            df_correlations = self.corr\n",
    "\n",
    "            pred = {\"topic_id\": [], \"content_ids\": []}\n",
    "            for i in range(len(df_topics)):\n",
    "                r_t = r[i]\n",
    "                tmp = []\n",
    "                for c in r_t:\n",
    "                    content_id = df_content.iloc[c][\"id\"]\n",
    "                    tmp.append(content_id)\n",
    "                topics_id = df_topics.iloc[i][\"id\"]\n",
    "                pred[\"topic_id\"].append(topics_id)\n",
    "                pred[\"content_ids\"].append(tmp)\n",
    "\n",
    "            df_pred = pd.DataFrame(pred).astype({\"topic_id\": str})\n",
    "            df_pred_list.append(df_pred)\n",
    "        df_pred = pd.concat(df_pred_list)\n",
    "        self.df_pred = df_pred\n",
    "        self.df_pred['content_ids'] = self.df_pred.apply(lambda x: ' '.join(x['content_ids']), axis=1)\n",
    "        \n",
    "    def save_pred(self, path='submission.csv'):\n",
    "        self.df_pred.to_csv(path, index=None)\n",
    "                \n",
    "    def retrieval(self):\n",
    "        self.get_embed()\n",
    "        self.inference()\n",
    "        self.save_pred()\n",
    "        return self.df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s1 = Retrieval(MODEL_DIR, TOKENIZER_DIR, dp)\n",
    "df_retrival = s1.retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416222ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrival = df_retrival.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrival.to_parquet(\"/home/search3/lichunyu/k12-curriculum-recommendations/data/output/stage2/valid/raw_submission.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.topic.to_parquet(\"/home/search3/lichunyu/k12-curriculum-recommendations/data/output/stage2/valid/topic.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.content.to_parquet(\"/home/search3/lichunyu/k12-curriculum-recommendations/data/output/stage2/valid/content.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2534e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrival[\"content_ids\"] = df_retrival[\"content_ids\"].apply(lambda x: x.split())\n",
    "df_retrival = df_retrival.explode(\"content_ids\").reset_index(drop=True)\n",
    "\n",
    "df_retrival = df_retrival.merge(dp.topic[[\"id\", \"field\"]], left_on=\"topic_id\", right_on=\"id\", how=\"left\")[\n",
    "    [\"topic_id\", \"content_ids\", \"field\"]\n",
    "].rename({\"field\": \"topic_field\"}, axis=1)\n",
    "df_retrival = df_retrival.merge(dp.content[[\"id\", \"field\"]], left_on=\"content_ids\", right_on=\"id\", how=\"left\")[\n",
    "    [\"topic_id\", \"content_ids\", \"topic_field\", \"field\"]\n",
    "].rename({\"field\": \"content_field\"}, axis=1)\n",
    "\n",
    "df_retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_parquet(corr_dir)\n",
    "label[\"label\"] = 1\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrival = df_retrival.merge(label, on=[\"topic_id\", \"content_ids\"], how=\"left\").fillna({\"label\": 0})\n",
    "df_retrival[\"label\"] = df_retrival[\"label\"].astype(\"int\")\n",
    "df_retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrival.to_parquet(\"/home/search3/lichunyu/k12-curriculum-recommendations/data/output/stage2/valid/valid.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c547a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6b7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43797800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed5ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/search3/miniconda3/envs/kaggle/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d97b25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020a0e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "2  3\n",
       "1  2\n",
       "0  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({\"a\": [1,2,3]}).sort_values(\"a\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904bed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1c820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148de95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63581f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
