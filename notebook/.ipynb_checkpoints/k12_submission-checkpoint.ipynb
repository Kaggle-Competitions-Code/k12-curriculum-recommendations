{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d800f070-f01a-486b-a1de-477abbaaa2d2",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0d4a3-ff6b-431f-bf8d-ad6f030cb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937dd37-d4ac-474b-9816-963ab7854c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '/root/autodl-nas/data/k12/cv_data/fold_0'\n",
    "OUTPUT_PATH = '.'\n",
    "\n",
    "TOPIC_DIR = os.path.join(INPUT_DIR, 'topics.csv')\n",
    "CONTENT_DIR = os.path.join(INPUT_DIR, 'content.csv')\n",
    "# CORR_DIR = os.path.join(INPUT_DIR, 'sample_submission.csv')\n",
    "CORR_DIR = os.path.join(INPUT_DIR, 'sample_submission.csv')\n",
    "\n",
    "MODEL_DIR = '/root/autodl-nas/model/r1_10/checkpoint-30816'\n",
    "TOKENIZER_DIR = '/root/autodl-nas/model/sentence-transformers/all-MiniLM-L6-v2_new_r1.1'\n",
    "\n",
    "N_NEIGHBOR = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe6cf5-fa92-46a5-8f56-53af5fbf8615",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81ddd4-c359-45bb-a88d-b1f81bdcef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_field(d):\n",
    "    title = list(filter(lambda x: pd.notna(x), d['title_level']))\n",
    "    title = ' of '.join(title[-1::-1])\n",
    "    title = 'No information' if title=='' else title\n",
    "    title = '[TITLE] ' + title + '. '\n",
    "    description = d['description'] if pd.notna(d['description']) else 'No information'\n",
    "    description = '[DESCRIPTION]' + description + '. '\n",
    "    field = title + description\n",
    "    return field\n",
    "\n",
    "def get_content_field(d):\n",
    "    title = d['title']\n",
    "    title = 'No information' if pd.isna(title) else title\n",
    "    title = '[TITLE] ' + title + '. '\n",
    "    description = d['description'] if pd.notna(d['description']) else 'No information'\n",
    "    description = '[DESCRIPTION]' + description + '. '\n",
    "    kind = '[' + d['kind'] + '] '\n",
    "    field = kind + title + description\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8264e2-bce9-49df-af9b-d6c8198ff668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    \n",
    "    def __init__(self, topic_path, content_path, submission_path):\n",
    "        self.topic = pd.read_csv(topic_path)\n",
    "        self.content = pd.read_csv(content_path)\n",
    "        self.corr = pd.read_csv(submission_path)\n",
    "        self.match_dict = None\n",
    "    \n",
    "    def prepare_topic(self):\n",
    "        df_level = self._get_level_features(self.topic)\n",
    "        self.topic = self.topic.merge(df_level, on='id', how='inner')\n",
    "        self.topic['field'] = self.topic.apply(lambda x: get_topic_field(x), axis=1)\n",
    "        return self.topic\n",
    "    \n",
    "    def prepare_content(self):\n",
    "        self.content['field'] = self.content.apply(lambda x: get_content_field(x), axis=1)\n",
    "        return self.content\n",
    "    \n",
    "    def prepare_language_match(self):\n",
    "        topic = self.topic[['id', 'language']].merge(self.corr, left_on='id', right_on='topic_id', how='right')[['id', 'language']]\n",
    "        match_dict = {}\n",
    "        for language in topic['language'].unique():\n",
    "            match_dict[language] = (topic.query('language==@language')[['id']], self.content.query('language==@language')[['id']])\n",
    "        self.match_dict = match_dict\n",
    "        return match_dict\n",
    "    \n",
    "    \n",
    "    def _get_level_features(self, df_topic, level_cols=['title']):\n",
    "        cols = list(set(level_cols + ['id', 'parent', 'level', 'has_content']))\n",
    "        df_hier = df_topic[cols]\n",
    "        \n",
    "        highest_level = df_hier['level'].max()\n",
    "        print(f'Highest Level: {highest_level}')\n",
    "\n",
    "        df_level = df_hier.query('level == 0').copy(deep=True)\n",
    "        level_list = list()\n",
    "        for col in level_cols:\n",
    "            df_level[f'{col}_level'] = df_level[f'{col}'].apply(lambda x: [x])\n",
    "\n",
    "        for i in tqdm(range(highest_level + 1)):\n",
    "            level_list.append(df_level[df_level['has_content']])\n",
    "            df_level_high = df_hier.query('level == @i+1')\n",
    "            df_level = df_level_high.merge(df_level, left_on='parent', right_on='id', suffixes=['', '_parent'], how='inner')\n",
    "            for col in level_cols:\n",
    "                df_level[f'{col}_level'] = df_level[f'{col}_level'] + df_level[f'{col}'].apply(lambda x: [x])\n",
    "            for col in df_level.columns:\n",
    "                if col.endswith('_parent'):\n",
    "                    df_level.drop(columns=col, inplace=True)\n",
    "        df = pd.concat(level_list).reset_index(drop=True)\n",
    "        return df[set(['id'] + [f'{col}_level' for col in level_cols])]\n",
    "    \n",
    "    def prepare(self):\n",
    "        self.prepare_topic()\n",
    "        self.prepare_content()\n",
    "        self.prepare_language_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a0b06-22d3-43a6-a2aa-3204ded843ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dp = DataPreparation(TOPIC_DIR, CONTENT_DIR, CORR_DIR)\n",
    "dp.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12674df3-298b-47ab-8a3b-6e9b0087a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, label_name=\"\") -> None:\n",
    "        super().__init__()\n",
    "        self.data = df[label_name].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        inputs = self.tokenizer(\n",
    "                text, \n",
    "                add_special_tokens = True,\n",
    "                truncation='longest_first',\n",
    "                max_length = 128,\n",
    "                padding = 'max_length',\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt',\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fecbd6-e9b6-46a4-903e-782f32cbf56f",
   "metadata": {},
   "source": [
    "## Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732db973-65ed-4014-ae96-94609e4dee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retrieval():\n",
    "    \n",
    "    def __init__(self, model_path, tokenizer_path, dp):\n",
    "        self.model = AutoModel.from_pretrained(model_path).cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        self.topic = dp.topic\n",
    "        self.content = dp.content\n",
    "        self.corr = dp.corr\n",
    "        self.topic_content_match = dp.match_dict\n",
    "    \n",
    "    def convert2embed(self, df, label_name='field'):\n",
    "        embed: list = []\n",
    "        dataset = PlainDataset(df, tokenizer=self.tokenizer, label_name=label_name)\n",
    "        dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=32)\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(**batch, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                embed.append(embeddings.cpu().clone().detach().numpy())\n",
    "        embed = np.concatenate(embed, axis=0)\n",
    "        return embed\n",
    "    \n",
    "    def get_embed(self):\n",
    "        for lang in self.topic_content_match.keys():\n",
    "            topic_, content_ = self.topic_content_match[lang]\n",
    "            topic_ = topic_[['id']].merge(self.topic[['id', 'field']], on='id', how='left')\n",
    "            content_ = content_[['id']].merge(self.content[['id', 'field']], on='id', how='left')\n",
    "            \n",
    "            topic_path = os.path.join(OUTPUT_PATH, f\"topic_{lang}.pqt\")\n",
    "            content_path = os.path.join(OUTPUT_PATH, f\"content_{lang}.pqt\")\n",
    "            topic_.to_parquet(topic_path)\n",
    "            content_.to_parquet(content_path)\n",
    "            \n",
    "            for t in [\"content\", \"topic\"]:\n",
    "                path = os.path.join(OUTPUT_PATH, f\"{t}_{lang}.pqt\")\n",
    "                df = pd.read_parquet(path)\n",
    "                embed = self.convert2embed(df, label_name=\"field\")\n",
    "                np.save(path.replace(\".pqt\", \".npy\"), embed)\n",
    "                \n",
    "    def inference(self):\n",
    "        recall_amount = 0\n",
    "        recall_amount_total = 0\n",
    "        recall_num = 0\n",
    "        recall_total = {}\n",
    "        \n",
    "        df_pred_list = []\n",
    "        for lang in self.topic_content_match.keys():\n",
    "            # global df_pred, df_correlations\n",
    "            content_path = os.path.join(OUTPUT_PATH, f\"content_{lang}.npy\")\n",
    "            topics_path = os.path.join(OUTPUT_PATH, f\"topic_{lang}.npy\")\n",
    "            content_array = np.load(content_path)\n",
    "            topics_array = np.load(topics_path)\n",
    "            \n",
    "            model = NearestNeighbors(n_neighbors=N_NEIGHBOR, metric=\"cosine\")\n",
    "            model.fit(content_array)\n",
    "            d, r = model.kneighbors(topics_array)\n",
    "            df_content = pd.read_parquet(content_path.replace(\".npy\", \".pqt\"))\n",
    "            df_topics = pd.read_parquet(topics_path.replace(\".npy\", \".pqt\"))\n",
    "            df_correlations = self.corr\n",
    "\n",
    "            pred = {\"topic_id\": [], \"content_ids\": []}\n",
    "            for i in range(len(df_topics)):\n",
    "                r_t = r[i]\n",
    "                tmp = []\n",
    "                for c in r_t:\n",
    "                    content_id = df_content.iloc[c][\"id\"]\n",
    "                    tmp.append(content_id)\n",
    "                topics_id = df_topics.iloc[i][\"id\"]\n",
    "                pred[\"topic_id\"].append(topics_id)\n",
    "                pred[\"content_ids\"].append(tmp)\n",
    "\n",
    "            df_pred = pd.DataFrame(pred).astype({\"topic_id\": str})\n",
    "            df_pred_list.append(df_pred)\n",
    "        df_pred = pd.concat(df_pred_list)\n",
    "        self.df_pred = df_pred\n",
    "        self.df_pred['content_ids'] = self.df_pred.apply(lambda x: ' '.join(x['content_ids']), axis=1)\n",
    "        \n",
    "    def save_pred(self, path='submission.csv'):\n",
    "        self.df_pred.to_csv(path, index=None)\n",
    "                \n",
    "    def retrieval(self):\n",
    "        self.convert2embed(self.topic[['id', 'field']])\n",
    "        self.get_embed()\n",
    "        self.inference()\n",
    "        self.save_pred()\n",
    "        \n",
    "    def sample_negative(self):\n",
    "        self.convert2embed(self.topic[['id', 'field']])\n",
    "        self.get_embed()\n",
    "        self.inference()\n",
    "        self.save_pred('samples_r3_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f2f0f-f7d0-48ff-b939-1b8c09c31ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06762ed9-c645-4576-8550-da51ab0bf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s1 = Retrieval(MODEL_DIR, TOKENIZER_DIR, dp)\n",
    "s1.sample_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524f63a-c464-49cb-a0c9-825ab2280d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33dfe8-5b36-497c-adaf-ecf97a506091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2efaf0-13b3-4008-950d-2dd9dbae11b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08b20d-03d9-4613-8d38-dd068569a9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
