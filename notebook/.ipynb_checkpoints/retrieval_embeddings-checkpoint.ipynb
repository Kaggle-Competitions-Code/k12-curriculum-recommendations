{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabb03ee-e2be-4d32-8f7a-75d0cbe84ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26e2738-8a45-4797-964f-f050a21c45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PATH:\n",
    "    # epoch 34668\n",
    "    model = '/root/autodl-nas/model/sentence-transformers/all-MiniLM-L6-v2_new_r1.1'\n",
    "    # input_dir = '/root/autodl-nas/data/k12'\n",
    "    input_dir = '/root/autodl-nas/data/k12/cv_data/fold_0'\n",
    "    \n",
    "    output_dir = '/root/autodl-nas/data/k12/out'\n",
    "    cv_dir = '/root/autodl-nas/data/k12/cv_data'\n",
    "    pretrained_dir = '/root/autodl-nas/model/'\n",
    "    content_dir = os.path.join(input_dir, 'content.csv')\n",
    "    correlation_dir = os.path.join(input_dir, 'correlations.csv')\n",
    "    submission_dir = os.path.join(input_dir, 'sample_submission.csv')\n",
    "    topic_dir = os.path.join(input_dir, 'topics.csv')\n",
    "    \n",
    "    \n",
    "class CFG:\n",
    "    seed = 11\n",
    "    fold = 0\n",
    "    n_fold = 3\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    # cpt = '/root/autodl-nas/model/checkpoint-34668/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d682abd-f04a-487e-9c30-9aad851e10d7",
   "metadata": {},
   "source": [
    "## Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b7b86e-1a5d-475b-b6db-218acec14691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_features(df_topic, level_cols=['title']):\n",
    "    df_hier = df_topic[list(set(level_cols + ['id', 'parent', 'level', 'has_content']))]\n",
    "    highest_level = df_hier['level'].max()\n",
    "    \n",
    "    df_level = df_hier.query('level == 0').copy(deep=True)\n",
    "    level_list = list()\n",
    "    for col in level_cols:\n",
    "        df_level[f'{col}_level'] = df_level[f'{col}'].apply(lambda x: [x])\n",
    "\n",
    "    for i in tqdm(range(highest_level + 1)):\n",
    "        level_list.append(df_level[df_level['has_content']])\n",
    "        df_level_high = df_hier.query('level == @i+1')\n",
    "        df_level = df_level_high.merge(df_level, left_on='parent', right_on='id', suffixes=['', '_parent'], how='inner')\n",
    "        for col in level_cols:\n",
    "            df_level[f'{col}_level'] = df_level[f'{col}_level'] + df_level[f'{col}'].apply(lambda x: [x])\n",
    "        for col in df_level.columns:\n",
    "            if col.endswith('_parent'):\n",
    "                df_level.drop(columns=col, inplace=True)\n",
    "    df = pd.concat(level_list).reset_index(drop=True)\n",
    "    return df[list(set(['id'] + [f'{col}_level' for col in level_cols]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9e16b7-0d49-4603-a1e1-9678beff122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_field(d):\n",
    "    title = list(filter(lambda x: pd.notna(x), d['title_level']))\n",
    "    title = ' of '.join(title[-1::-1])\n",
    "    title = 'No information' if title=='' else title\n",
    "    title = '[TITLE] ' + title + '. '\n",
    "    description = d['description'] if pd.notna(d['description']) else 'No information'\n",
    "    description = '[DESCRIPTION]' + description + '. '\n",
    "    field = title + description\n",
    "    return field\n",
    "\n",
    "def get_content_field(d):\n",
    "    title = d['title']\n",
    "    title = 'No information' if pd.isna(title) else title\n",
    "    title = '[TITLE] ' + title + '. '\n",
    "    description = d['description'] if pd.notna(d['description']) else 'No information'\n",
    "    description = '[DESCRIPTION]' + description + '. '\n",
    "    kind = '[' + d['kind'] + '] '\n",
    "    field = kind + title + description\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f05127c-7b3c-4898-a13d-8860aa46d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_language_match(path, mode='train'):\n",
    "    topic = pd.read_csv(path.topic_dir)[['id', 'language']]\n",
    "    content = pd.read_csv(path.content_dir)[['id', 'language']]\n",
    "    if mode == 'train':\n",
    "        corr = pd.read_csv(path.correlation_dir)\n",
    "    elif mode == 'valid':\n",
    "        corr = pd.read_csv(path.submission_dir)\n",
    "    \n",
    "    topic = topic.merge(corr, left_on='id', right_on='topic_id', how='right')[['id', 'language']]\n",
    "    match_dict = {}\n",
    "    for language in topic['language'].unique():\n",
    "        match_dict[language] = (topic.query('language==@language')[['id']], content.query('language==@language')[['id']])\n",
    "    return match_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e466c85c-a08a-43ac-880e-e968453a169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_match_features(topic, content, path):\n",
    "    df_topic = pd.read_csv(path.topic_dir)\n",
    "    df_content = pd.read_csv(path.content_dir)\n",
    "    level = get_level_features(df_topic)\n",
    "    df_topic = df_topic.merge(level, on='id', how='right')\n",
    "    df_topic['field'] = df_topic.apply(lambda x: get_topic_field(x), axis=1)\n",
    "    df_content['field'] = df_content.apply(lambda x: get_content_field(x), axis=1)\n",
    "    topic = topic[['id']].merge(df_topic[['id', 'field']], on='id', how='left')\n",
    "    content = content[['id']].merge(df_content[['id', 'field']], on='id', how='left')\n",
    "    return topic, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab94b55-ec29-4859-84cf-aae02c197399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 1.31 s, total: 11.6 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topic_content_match = prepare_language_match(PATH, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d967897c-6328-4508-a449-409f27d86aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gu\t - topics: 131\t - contents: 3677\n",
      "en\t - topics: 5373\t - contents: 65939\n",
      "es\t - topics: 2189\t - contents: 30844\n",
      "fr\t - topics: 111\t - contents: 10682\n",
      "hi\t - topics: 132\t - contents: 4042\n",
      "fil\t - topics: 83\t - contents: 516\n",
      "pt\t - topics: 76\t - contents: 10435\n",
      "bn\t - topics: 191\t - contents: 2513\n",
      "as\t - topics: 18\t - contents: 641\n",
      "sw\t - topics: 31\t - contents: 1447\n",
      "CPU times: user 525 µs, sys: 0 ns, total: 525 µs\n",
      "Wall time: 419 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lang in topic_content_match.keys():\n",
    "    print(f'{lang}\\t - topics: {len(topic_content_match[lang][0])}\\t - contents: {len(topic_content_match[lang][1])}')\n",
    "    topic, content = topic_content_match[lang]\n",
    "    topic, content = prepare_match_features(topic, content, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938fd06-20e7-4575-a5e2-5444486a1525",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calc Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733362a1-5df4-4895-8063-92e17141245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "MODEL_PATH = PATH.model\n",
    "OUTPUT_PATH = os.path.join(PATH.cv_dir, f\"fold_{CFG.fold}\")\n",
    "N_NEIGHBOR = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "468cf7f2-a99f-424c-9e38-69d436c58d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, label_name=\"\") -> None:\n",
    "        super().__init__()\n",
    "        self.data = df[label_name].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        inputs = self.tokenizer(\n",
    "                text, \n",
    "                add_special_tokens = True,\n",
    "                truncation='longest_first',\n",
    "                max_length = 64,\n",
    "                padding = 'max_length',\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt',\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92794b6-f21c-470f-b665-fe2218beb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert2Embed(object):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModel.from_pretrained(MODEL_PATH).cuda()\n",
    "\n",
    "    def convert2embeddind(self, df, label_name=\"\"):\n",
    "        embed: list = []\n",
    "        dataset = PlainDataset(df, tokenizer=self.tokenizer, label_name=label_name)\n",
    "        dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=32)\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.model(**batch, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                embed.append(embeddings.cpu().clone().detach().numpy())\n",
    "        embed = np.concatenate(embed, axis=0)\n",
    "        return embed\n",
    "\n",
    "    def get_embed(self):\n",
    "        for lang in topic_content_match.keys():\n",
    "            # topic, content = topic_content_match[lang]\n",
    "            # topic, content = prepare_match_features(topic, content, PATH)\n",
    "            # topic_path = os.path.join(OUTPUT_PATH, \"valid\", f\"topic_{lang}.pqt\")\n",
    "            # content_path = os.path.join(OUTPUT_PATH, \"valid\", f\"content_{lang}.pqt\")\n",
    "            # topic.to_parquet(topic_path)\n",
    "            # content.to_parquet(content_path)\n",
    "            \n",
    "            for t in [\"content\", \"topic\"]:\n",
    "                path = os.path.join(OUTPUT_PATH, \"valid\", f\"{t}_{lang}.pqt\")\n",
    "                df = pd.read_parquet(path)\n",
    "                embed = self.convert2embeddind(df, label_name=f\"field\")\n",
    "                np.save(path.replace(\".pqt\", \".npy\"), embed)\n",
    "                \n",
    "        # for p in tqdm(valid_language):\n",
    "        #     for t in [\"content\", \"topics\"]:\n",
    "        #         path = os.path.join(OUTPUT_PATH, \"valid\", p, f\"{t}_{p}.pqt\")\n",
    "        #         df = pd.read_parquet(path)\n",
    "        #         embed = self.convert2embeddind(df, label_name=f\"{t}_text\")\n",
    "        #         np.save(path.replace(\".pqt\", \".npy\"), embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f08b9ef-56fe-44bd-8f61-1f54f6d51c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "#     with open(os.path.join(OUTPUT_PATH, \"valid\", \"language.txt\"), \"r\") as f:\n",
    "#         valid_language = f.read().splitlines()\n",
    "    recall_amount = 0\n",
    "    recall_num = 0\n",
    "    recall_total = {}\n",
    "    for lang in topic_content_match.keys():\n",
    "        ## debug\n",
    "        # global df_pred, df_correlations\n",
    "        content_path = os.path.join(OUTPUT_PATH, \"valid\", f\"content_{lang}.npy\")\n",
    "        topics_path = os.path.join(OUTPUT_PATH, \"valid\", f\"topic_{lang}.npy\")\n",
    "        correlations_path = PATH.submission_dir\n",
    "        content_array = np.load(content_path)\n",
    "        topics_array = np.load(topics_path)\n",
    "        model = NearestNeighbors(n_neighbors=N_NEIGHBOR, metric=\"cosine\")\n",
    "        model.fit(content_array)\n",
    "        d, r = model.kneighbors(topics_array)\n",
    "        df_content = pd.read_parquet(content_path.replace(\".npy\", \".pqt\"))\n",
    "        df_topics = pd.read_parquet(topics_path.replace(\".npy\", \".pqt\"))\n",
    "        df_correlations = pd.read_csv(correlations_path).astype({\"topic_id\": str})\n",
    "        # .rename({\"topic_id\": \"topics_id\", \"content_ids\": \"content_id\"}, axis=1)\n",
    "        \n",
    "        pred = {\"topic_id\": [], \"content_ids\": []}\n",
    "        for i in range(len(df_topics)):\n",
    "            r_t = r[i]\n",
    "            tmp = []\n",
    "            for c in r_t:\n",
    "                content_id = df_content.iloc[c][\"id\"]\n",
    "                tmp.append(content_id)\n",
    "            topics_id = df_topics.iloc[i][\"id\"]\n",
    "            pred[\"topic_id\"].append(topics_id)\n",
    "            pred[\"content_ids\"].append(tmp)\n",
    "        \n",
    "        df_pred = pd.DataFrame(pred).astype({\"topic_id\": str})\n",
    "        df_correlations['content_ids'] = df_correlations['content_ids'].apply(lambda x: list(x.split()))\n",
    "        df_pred = df_pred.merge(df_correlations, on='topic_id', how='left', suffixes=['_pred', '_true'])\n",
    "        \n",
    "        df_pred['recall'] = df_pred.apply(lambda x: len(set(x['content_ids_true']).intersection(x['content_ids_pred']))\n",
    "                                                       / len(x['content_ids_true']), \n",
    "                                          axis=1)\n",
    "        recall = df_pred['recall'].mean()\n",
    "        recall_total[lang] = recall\n",
    "        recall_num += len(df_pred)\n",
    "        recall_amount += df_pred['recall'].sum()\n",
    "    print(f\"Recall: {recall_amount/recall_num}\")\n",
    "    print(f\"----------------Details----------------\")\n",
    "    for k, v in recall_total.items():\n",
    "        print(f\"Recall for language {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c004a5f-6bf1-4cb1-b2b9-95f758935e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # P = Convert2Embed()\n",
    "    # P.get_embed()\n",
    "    N_NEIGHBOR = 50\n",
    "    valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e67346-1c11-45c8-85c8-a93407f8d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # P = Convert2Embed()\n",
    "    # P.get_embed()\n",
    "    N_NEIGHBOR = 100\n",
    "    valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b6c6674-9d0d-4cd1-bc71-f81fa2cd014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8091437845276167\n",
      "----------------Details----------------\n",
      "Recall for language gu: 0.5162500717442461\n",
      "Recall for language en: 0.9077353249369697\n",
      "Recall for language es: 0.5824658411831884\n",
      "Recall for language fr: 0.8474526585207699\n",
      "Recall for language hi: 0.7574651578002543\n",
      "Recall for language fil: 0.9781985083189902\n",
      "Recall for language pt: 0.8899573744968482\n",
      "Recall for language bn: 0.7173052514675551\n",
      "Recall for language as: 1.0\n",
      "Recall for language sw: 0.8522539288668322\n",
      "CPU times: user 1min 45s, sys: 16.8 s, total: 2min 2s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # P = Convert2Embed()\n",
    "    # P.get_embed()\n",
    "    N_NEIGHBOR = 200\n",
    "    valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d97fe-c8e3-409d-8d08-7f77201ef940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c211e8-1c87-4aa4-9af3-0f36faa98e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
